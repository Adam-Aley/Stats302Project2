---
title: "Project 2: Stats302Project2 Tutorial"
author: "Adam Aley" 
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Stats302Project2 Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Here is \texttt{Stats302Project2}, a package containing four main functions 
and tutorials for my Stat 302 Final Project. These four functions include:

- A t-test function, 'my_test'
- A linear regression function 'my_lm'
- A k-nearest neighbors cross-validation function 'my_knn_cv'
- A random forest cross-validation function 'my_rf_cv'

## Installation

To download the \texttt{Stats302Project2} package, use the code below.

Install \texttt{Stats302Project2} using:

```{r, eval = FALSE}
devtools::install_github("Adam-Aley/Stats302Project2")
```

Here, we load our example datasets as a \texttt{gapminder} object:
```{r, message = FALSE}
library(randomForest)
library(Stats302Project2)
library(ggplot2)
library(dplyr)
library(testthat)
library(class)
data("my_penguins")
new_penguins <- na.omit(my_penguins)
```

# Demonstrating t-test

All the following tests use the lifeExp data from 'my_gapminder'.
All our results will be compared against a p-value cut-off of $\alpha = 0.05$.

First, we will demonstrate a test of the hypothesis:
$H_0 : \mu = 60$
$H_A : \mu \neq 60$
```{r}
# Runs t-test
my_test(my_gapminder$lifeExp, "two-sided", 60)
```

We cannot reject the null hypothesis compared against a significance level of
$\alpha = 0.05$, as p-value > 0.05.

Next, we will demonstrate a test of the hypothesis:
$H_0 : \mu = 60$
$H_A : \mu < 60$
```{r}
# Runs t-test
my_test(my_gapminder$lifeExp, "lesser", 60)
```

We can reject the null hypothesis compared against a significance level 
of $\alpha = 0.05$, as p-value < 0.05.

Finally, we will demonstrate a test of the hypothesis:
$H_0 : \mu = 60$
$H_A : \mu > 60$
```{r}
# Runs t-test
my_test(my_gapminder$lifeExp, "greater", 60)
```

We cannot reject the null hypothesis compared against a significance level of
$\alpha = 0.05$, as p-value > 0.05.


# Demonstrating linear regression function

Using 'lifeExp' data as the response variable and 'gdpPercap' and continent as 
explatory variables, we can create a linear regression model to test for 
correlation. 

```{r}
#Runs linear regression function, creates model for inputs
my_lm(lifeExp ~ gdpPercap + continent, my_gapminder)
```


The 'gdpPercap' coefficient represents the change in GDP per Capita with a 
corresponding change of 1 unit in Life Expectancy, seperated by 'continents'.

The hypothesis test associated with the gdpPercap coefficient is:
$H_0 : GDP \ per \ capita \  does \ not \ affect \ Life Expectancy$
$H_A : GDP \ per \ capita \ does \ affect \ Life Expectancy$

We can reject the null hypothesis compared against a significance level 
of $\alpha = 0.05$, as p-value < 0.05 by a significant amount. Therefore,
GDP per capital does have a significant correlation to Life Expectancy.

Here, we plot the Actual values against the Fitted values to get a sense of how
strong our interpretation is:

```{r}
# Generates model
lm <- my_lm(lifeExp ~ gdpPercap + continent, my_gapminder)
# Creates prediction points from model
model <- model.matrix(lifeExp ~ gdpPercap + continent,
                      my_gapminder)%*%lm$Estimate
# Plots actual values against predictions
ggplot(data = cbind(my_gapminder, model) %>% rename(Continent = continent), 
       aes(x = model, y = lifeExp, color = Continent) ) +
  # Creates plots
  geom_point() +
  # Sets lables
  labs(title = "Actual vs. Fitted Life Expectancy", 
       x = "Fitted", y = "Actual") +
  # Resets theme
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```

Though the correlation between GDP per capita and Life Expectancy isn't perfect,
with greater deviations for more populated continents, the plot generally
reflects our previous assumption of a strong correlation between the two variables. 

# Demonstrating K-Nearest Neighbors Cross-Validation Function

Using 'K-Nearest Neighbors', we can predict the output of species with covariates 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', and 'body_mass_g'. 
Using 5-fold cross validation, and iterating with a 'k_nn' ranging from 1:10, we
we can evaluate each prediction's 'training error' and 'cross-validation error' as 
well. 

```{r}
data("my_penguins")
new_penguins <- na.omit(my_penguins)
# Create empty 10 unit vector
cv <- numeric(10)
# Create another empty 
train <- numeric(10)
# Stores cv and training errors for 5 fold k_nearest10 unit vector neighbors from 1 to 10
for (n in 1:10) {
  cv[n] <- my_knn_cv(new_penguins[, -c(1, 2, 7, 8)], new_penguins$species, n, 5)$cv_err
  train[n] <- my_knn_cv(new_penguins[, -c(1, 2, 7, 8)], new_penguins$species, n, 5)$train_err
}
cbind(cv, train)
``` 

If I were to choose a model purely based on training misclassification rate, I would pick k = 1 due to the fact that it has a flawless classification rate. If I were to choose a model purely based on the cross-validation misclassification rate, I would choose $k = 10$ because, while it does not have the lowest misclassification rate, it would serve a better use for inputs not based off the training data. The 'training error' data has high bias but low variability since it is based on the model we compare other input data to. On the other hand, cross-validation evaluates models against the training error with lower bias but high variance in order to better predict future inputs. 

# Demonstrating Random Forest Cross-Validation Function

```{r}
# Initialize three length 30 vectors
CV_2 <- 1:30
CV_5 <- 1:30
CV_10<- 1:30
# Fill vectors with random cv errors of various random forests of same folds
for (k in 1:30) {
  CV_2[k] <- my_rf_cv(new_penguins, 2)$cv_err
  CV_5[k] <- my_rf_cv(new_penguins, 5)$cv_err
  CV_10[k] <- my_rf_cv(new_penguins, 10)$cv_err 
}
```

```{r}
# Groups all CVs
CVs <- cbind(CV_2, CV_5, CV_10)
# Categorize CVs by number of folds
folds <- rep(c("2", "5", "10"), each = 30)
final_folds <- as.factor(folds)
df <- data.frame(CVs, final_folds)
# Create boxplot of various numbers of folds
ggplot(data = as.data.frame(cbind(as.numeric(CVs), final_folds)), 
       aes(x = final_folds, y = CVs, group = final_folds)) +
  # Generates plot
  geom_boxplot() +
  # Creates lables
  labs(title = "Cross-Validation Errors of Multiple Random Forests", 
       x = "Total Folds (2, 10, 5)", y = "Cross-Validation Error") +
  # Resets themes
  theme(plot.title = element_text(hjust = 0.5, face = "bold"), 
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
        panel.spacing.x = unit(0.75, "cm")) 
# Gives sensible order to boxes of folds
  scale_x_discrete(limits = c("k = 2", "k = 5", "k = 10"))
```



```{r}
# Creates vector of mean CVs for various folds
average_cv <- c(mean(CVs[1:30]), mean(CVs[31:60]), mean(CVs[61:90]))
# Creates vector of sd of CVs for various folds
sd_cv <- c(sd(CVs[1:30]), sd(CVs[31:60]), sd(CVs[61:90]))
# Produces table of means and sds for various folds
base <- as.data.frame(cbind(average_cv, sd_cv))
rownames(base) <- c("10 folds", "5 folds", "2 folds")
base
```

As the number of folds increase between each 'random forest test', we see a corresponding decrease in both the mean and standard deviation. With 2 total folds, we see a boxplot with a relatively high range compared to the boxplots of 5 and 10 folds, reflecting that more and more folds lead to higher accuracy and a smaller range of error. 





